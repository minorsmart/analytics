[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analytics in Python",
    "section": "",
    "text": "1 Data Understanding\n\n\nLinks:\n\nwww.kaggle.com/\ndatasetsearch.research.google…\ndata.fivethirtyeight.com/\ndata.gov/\ngithub.com/search?q=dataset\ndata.nasa.gov/\nselected datasets\n\n\n\n\nLesson outline\n\n\n\n\n\n\n\n\n\n\n\nTopic\nTasks\nActivities\nStudent\nTeacher\n\n\n\n\n1\nFinding Data\nExplore the different sources of data that may be used in data mining, and how to extract and access this data.\nThink-Pair-Share: students will individually brainstorm potential sources of data, pair up with a partner to discuss, and then share with the class.\n‘We learned about various data sources and perspectives of different students during the brainstorming activity.’\n‘Our objective here is to generate a list of possible sources of data that we can use for data mining. As a teacher, I want you to participate actively in brainstorming and support each other’s thoughts. As students, you will be able to collaborate and gain insights from your peers.’\n\n\n2\nDescriptive Statistics\nCalculate basic descriptive statistics that are commonly used in data mining, and understand how they are used to summarize datasets.\nJigsaw: students will be grouped into teams and tasked to gather data from various sources, conduct descriptive statistics, and report their findings to the rest of the class.\n‘We learned the importance of teamwork, critical thinking, and communication skills by working together to conduct descriptive statistics on our assigned data set.’\n‘The goal here is to give every student a chance to delve deeper into specific aspects of data mining. As a teacher, my role is to facilitate the group and ensure everyone is participating. As students, you are expected to synthesize, analyze, and present your findings through a collaborative effort.’\n\n\n\n\n\nETL stands for Extract, Transform, and Load. It is a process used in data integration and data warehousing to extract data from various sources, transform it into a consistent format, and load it into a target system. The “Extract” phase involves gathering data from multiple sources, such as databases, spreadsheets, or APIs. In the “Transform” phase, the data is cleaned, validated, and standardized to ensure consistency and quality. Finally, in the “Load” phase, the transformed data is loaded into a target system, such as a data warehouse or a business intelligence tool, making it accessible for analysis and reporting. ETL is a crucial step in data management to ensure accurate and usable data for decision-making.\nIn the next example we are going to extract and load a data set. The data comes from here.\n\nimport pandas as pd\nimport plotly.express as px\n\n# Extract\nurl = \"https://raw.githubusercontent.com/businessdatasolutions/courses/main/datamining-n/datasets/ai.csv\"\nrawDF = pd.read_csv(url)\n# Transform\nrawDF = rawDF.sort_values(by=['year', 'Domain'])\nrawDF.head()\n\n                     Entity  Code  ...  Training_computation_petaflop  Domain\n216                 Theseus   NaN  ...                   4.000000e-14   Other\n195                   SNARC   NaN  ...                            NaN   Other\n198  Self Organizing System   NaN  ...                            NaN  Vision\n167       Perceptron Mark I   NaN  ...                   6.950000e-10  Vision\n197  Samuel Neural Checkers   NaN  ...                   4.280000e-07   Games\n\n[5 rows x 6 columns]\n\n\nOnce you have accessed your dataset you’ll want to get familiar with the content and gain insights into its quality and structure. Data analysts or data scientists collect and examine the data to understand its relevance to the project’s goals. They explore the data using various techniques, such as descriptive statistics, data visualization, and data profiling. The goal is to identify patterns, relationships, and potential issues within the dataset, which helps in formulating initial hypotheses and refining the project’s objectives.\n\n# Load\nfig = px.scatter(rawDF, x=\"year\", y=\"Training_computation_petaflop\", color=\"Domain\", log_y=True, hover_name=\"Entity\", title=\"Computation used to train notable artificial intelligence systems\")\nfig.update_traces(marker={'size': 12})\n\n\n                        \n                                            \n\nrawDF.describe()\n\n       Code         year  Training_computation_petaflop\ncount   0.0   246.000000                   1.040000e+02\nmean    NaN  2009.939024                   3.002598e+08\nstd     NaN    15.437310                   2.084483e+09\nmin     NaN  1950.000000                   4.000000e-14\n25%     NaN  2010.000000                   3.650000e+00\n50%     NaN  2015.500000                   2.405000e+04\n75%     NaN  2019.000000                   1.100000e+07\nmax     NaN  2023.000000                   2.100000e+10"
  },
  {
    "objectID": "nb.html#business-case-filtering-spam",
    "href": "nb.html#business-case-filtering-spam",
    "title": "2  Probabilistic Learning with Naive Bayes Classification",
    "section": "2.1 Business Case: Filtering Spam",
    "text": "2.1 Business Case: Filtering Spam\nIn 2020 spam accounted for more than 50% of total e-mail traffic [@noauthor_spam_nodate]. This illustrates the value of a good spam filter. Naive Bayes spam filtering is a standard technique for handling spam. It is one of the oldest ways of doing spam filtering, with roots in the 1990s."
  },
  {
    "objectID": "nb.html#data-understanding",
    "href": "nb.html#data-understanding",
    "title": "2  Probabilistic Learning with Naive Bayes Classification",
    "section": "2.2 Data Understanding",
    "text": "2.2 Data Understanding\nThe data you’ll be using comes from the SMS Spam Collection [@noauthor_uci_spam_nodate]. It contains a set of SMS messages that are labeled ‘ham’ or ‘spam’. and is a standard data set for testing spam filtering methods.\n\nurl = \"https://raw.githubusercontent.com/businessdatasolutions/courses/main/datamining-n/datasets/smsspam.csv\"\nrawDF = pd.read_csv(url)\nrawDF.head()\n\n   type                                               text\n0   ham  Go until jurong point, crazy.. Available only ...\n1   ham                      Ok lar... Joking wif u oni...\n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3   ham  U dun say so early hor... U c already then say...\n4   ham  Nah I don't think he goes to usf, he lives aro...\n\n\nThe variable type is of class object which in Python refers to text. As this variable indicates whether the message belongs to the category ham or spam it is better to convert it to a category variable.\n\ncatType = CategoricalDtype(categories=[\"ham\", \"spam\"], ordered=False)\nrawDF.type = rawDF.type.astype(catType)\nrawDF.type\n\n0        ham\n1        ham\n2       spam\n3        ham\n4        ham\n        ... \n5567    spam\n5568     ham\n5569     ham\n5570     ham\n5571     ham\nName: type, Length: 5572, dtype: category\nCategories (2, object): ['ham', 'spam']\n\n\nTo see how the types of sms messages are distributed you can compare the counts for each category.\n\nrawDF.type.value_counts()\n\ntype\nham     4825\nspam     747\nName: count, dtype: int64\n\n\nOften you’ll prefer the relative counts.\n\nrawDF.type.value_counts(normalize=True)\n\ntype\nham     0.865937\nspam    0.134063\nName: proportion, dtype: float64\n\n\nYou can also visually inspect the data by creating wordclouds for each sms type.\n\n# Generate a word cloud image]\nhamText = ' '.join([Text for Text in rawDF[rawDF['type']=='ham']['text']])\nspamText = ' '.join([Text for Text in rawDF[rawDF['type']=='spam']['text']])\ncolorListHam=['#e9f6fb','#92d2ed','#2195c5']\ncolorListSpam=['#f9ebeb','#d57676','#b03636']\ncolormapHam=colors.ListedColormap(colorListHam)\ncolormapSpam=colors.ListedColormap(colorListSpam)\nwordcloudHam = WordCloud(background_color='white', colormap=colormapHam).generate(hamText)\nwordcloudSpam = WordCloud(background_color='white', colormap=colormapSpam).generate(spamText)\n\n# Display the generated image:\n# the matplotlib way:\nfig, (wc1, wc2) = plt.subplots(1, 2)\nfig.suptitle('Wordclouds for ham and spam')\nwc1.imshow(wordcloudHam)\nwc2.imshow(wordcloudSpam)\nplt.show()\n\n\n\n\nQuestion:\n\nWhat differences do you notice?"
  },
  {
    "objectID": "nb.html#preparation",
    "href": "nb.html#preparation",
    "title": "2  Probabilistic Learning with Naive Bayes Classification",
    "section": "2.3 Preparation",
    "text": "2.3 Preparation\nAfter you’ve glimpsed over the data and have a certain understanding of its structure and content, you are now ready to prepare the data for further processing. For the naive bayes model you’ll need to have a dataframe with wordcounts. To save on computation time you can set a limit on the number of features (columns) in the wordsDF dataframe.\n\nvectorizer = TfidfVectorizer(max_features=1000)\nvectors = vectorizer.fit_transform(rawDF.text)\nwordsDF = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names_out())\nwordsDF.head()\n\n   000   03   04  0800  08000839402  ...  your  yours  yourself   yr  yup\n0  0.0  0.0  0.0   0.0          0.0  ...   0.0    0.0       0.0  0.0  0.0\n1  0.0  0.0  0.0   0.0          0.0  ...   0.0    0.0       0.0  0.0  0.0\n2  0.0  0.0  0.0   0.0          0.0  ...   0.0    0.0       0.0  0.0  0.0\n3  0.0  0.0  0.0   0.0          0.0  ...   0.0    0.0       0.0  0.0  0.0\n4  0.0  0.0  0.0   0.0          0.0  ...   0.0    0.0       0.0  0.0  0.0\n\n[5 rows x 1000 columns]\n\n\nThe counts are normalized in such a way that the words that are most likely to have predictive power get heavier weights. For instance stopword like “a” and “for” most probably will equally likely feature in spam as in ham messages. Therefore these words will be assigned lower normalized counts.\nBefore we start modeling we need to split all datasets into train and test sets. The function train_test_split() can be used to create balanced splits of the data. In this case we’ll create a 75/25% split.\n\nxTrain, xTest, yTrain, yTest = train_test_split(wordsDF, rawDF.type)"
  },
  {
    "objectID": "nb.html#modeling-and-evaluation",
    "href": "nb.html#modeling-and-evaluation",
    "title": "2  Probabilistic Learning with Naive Bayes Classification",
    "section": "2.4 Modeling and Evaluation",
    "text": "2.4 Modeling and Evaluation\nWe have now everything in place to start training our model and evaluate against our test dataset. The MultinomialNB().fit() function is part of the scikit learn package. It takes in the features and labels of our training dataset and returns a trained naive bayes model.\n\nbayes = MultinomialNB()\nbayes.fit(xTrain, yTrain)\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()\n\n\nThe model can be applied to the test features using the predict() function which generates a array of predictions. Using a confusion matrix we can analyze the performance of our model.\n\n\n\n\n\nStandard diffusion table. Taken from: https://emj.bmj.com/content/emermed/36/7/431/F1.large.jpg\n\n\n\n\n\nyPred = bayes.predict(xTest)\nyTrue = yTest\n\n\naccuracyScore = accuracy_score(yTrue, yPred)\nprint(f'Accuracy: {accuracyScore}')\n\nAccuracy: 0.9777458722182341\n\nmatrix = confusion_matrix(yTrue, yPred)\nlabelNames = pd.Series(['ham', 'spam'])\npd.DataFrame(matrix,\n     columns='Predicted ' + labelNames,\n     index='Is ' + labelNames)\n\n         Predicted ham  Predicted spam\nIs ham            1205               2\nIs spam             29             157\n\n\nQuestions:\n\nWhat do you think is the role of the alpha parameter in the MultinomialNB() function?\nHow would you assess the overall performance of the model?\nWhat would you consider as more costly: high false negatives or high false positives levels? Why?"
  }
]